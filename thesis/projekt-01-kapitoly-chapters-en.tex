% This file should be replaced with your file with thesis content.
%=========================================================================
% Authors: Michal Bidlo, Bohuslav KÅ™ena, Jaroslav Dytrych, Petr Veigend and Adam Herout 2019

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}

\chapter{Note to the Reader}
Through the thesis, the expression "iff" refers to "if and only if".\\
Except if explicitly stated otherwise, the following notation conventions apply throughout the thesis. 
In the context of regular languages, lower case Latin alphabet letters, such as $a, b, c, \dots$, refer to terminal symbols. Upper case Latin alphabet letters, such as $A, B, C,\dots$, denote nonterminal symbols.

\chapter{Introduction}
In modern society humans are reliant on a multitude of technologies such as internet, mobile phones, television, radio, microwave ovens, camera sensors, lasers, light emitting diodes, electrical motors, medical imaging systems and many others. All of these are electromagnetic devices, thus without a doubt electromagnetism plays a key role in day-to-day life.

Electromagnetic field theory is the underpinning framework used to study the effects of electromagnetic phenomena at scales where quantum effects are negligible. It studies the interactions between electric charges and currents (Currents are often referred to as electric charges in motion). Maxwell's equations, a set of fundamental coupled partial differential equations form the cornerstone of classical electromagnetism. Closed form analytical solutions are complex, know for only simple cases and usually inapplicable in real world use.

However with the rapid increase of available computational power, the use of analytically simple but computationally taxing methods arose as they became more available. These method provide solutions to more general problems compared to their counterparts. The branch of electromagnetics that focuses on such methods is know as computational electromagnetics (CEM). CEM allows for simulating more complex problems and verifying designs before production of prototypes. They provide key insight in to the operation of electromagnetic devices and even certain information that may be unattainable by classical analytical methods. More over with the ability to tweak parameters and resimulate, CEM brings about an advent of design optimization in electromagnetic devices.

Today, there are many such tools and software packages, some commercial and others open-source. It is important to be able to verify results against other implementations, enabling researchers to verify that results are not artifacts of a particular solver but are physically meaningful. Furthermore tools may offer different features and constrains, thus interoperability is desired. This thesis explores the possibility of translating simulation code between two such tools, Ansys Lumerical and Meep.


\todo{Add motivation for thesis, talk about importance of optics simulations, talk about uses, show examples. Talk about the need to compare simulation software.}

\chapter{Review of Relevant Literature}
This chapter aims to give the reader a broad understanding of the subjects of matter. It provides an overview of the physics behind the aforementioned simulations and an introduction to the basic concepts of compiler design and the underlying formal language theory. This chapter will not delve into the specifics of each subject at hand but will provide the reader with the foundations necessary to comprehend the rest of this thesis.

\section{Computational Electromagnetics}
\subsection*{Curl theorem}
\subsection*{Divergence theorem}
\subsection*{Maxwell's equations}
\todo{show all 4 equations}
\todo{derive partial form form integral form}



\subsection*{Finite Difference Time Domain}
\section{Formal Language Theory}
We use languages like English or Slovak every day to communicate information, these languages are referred to as \emph{natural languages}. The Oxford English Dictionary defines language as a system of spoken or written communication used by a particular country, people, community. How ever a more rigorous definition of languages and the tools to operate on them is required. That is what the notion of formal language theory will introduce in the following sections.

\subsection{Alphabets and Languages}

\todo{add text about languages and alphabets, use English as example}

\begin{definition}[Alphabet]
\label{def:alphabet}
Let $\Sigma$ be an \emph{alphabet}, a finite nonempty set of symbols, letters. Then $\Sigma ^{*}$ defines the set of all sequences $w$:
$$w= a_1 a_2 a_3 \dots a_{n-1} a_n, \in \Sigma \text{ for } n \in \mathbb{N}$$
\end{definition}

The sequence of symbols $w$ is called a \emph{word}. The length of a word is given by the number of symbols $a$, symbolically $|w| = n$. The word with a length of 0 is called an \emph{empty word} denoted as $\epsilon$.

\begin{definition}[Language]
\label{def:language}
The set $L$ where $L\subseteq \Sigma^{*}$ is know as the \emph{formal language} over the alphabet $\Sigma$. 
\end{definition}
The words $L = \left\lbrace \epsilon, a, b, aa, ab, bb \right\rbrace$ are some words of a language $L$ over the alphabet $\Sigma=\left\lbrace a,b \right\rbrace$.
Other examples for languages over the alphabet $\Sigma=\left\lbrace a,b \right\rbrace$ might include:
\begin{itemize}
\item $L_1 = \left\lbrace \epsilon \right\rbrace$
\item $L_2 = \left\lbrace a \right\rbrace$
\item $L_3 = \left\lbrace aaa \right\rbrace$
\item $L_4 = \left\lbrace a^i,b^i; i \in \mathbb{N} \right\rbrace$
\end{itemize}




\paragraph*{Notation conventions}



\begin{itemize}
\item Iteration\\
  Let $a^i; a \in \Sigma \text{ and } i \in \mathbb{Z}$ be the \emph{iteration} of a character $a$, where $|a^i| = i$.\\
Example bellow:


\begin{itemize}
\item $a^0 = \epsilon$
\item $a^1 = a$
\item $a^2 = aa$
\item $a^i = a_0 a_1 a_2 \dots a_i; i \in \mathbb{N}$
\end{itemize}



\item Concatenation\\
  Let $w \cdot w^{'}; w, w^{'} \in \Sigma^{*}$ be the \emph{concatenation} of the words $w$ and $w^{'}$.

$w = a_1 a_2 a_3 \dots a_n ; w^{'} = a^{'}_1 a^{'}_2 a^{'}_3 \dots a^{'}_m; n,m \in \mathbb{Z}$ then  

$w\cdot w^{'} = w w^{'} = a_1 a_2 a_3 \dots a_n a^{'}_1 a^{'}_2 a^{'}_3 \dots a^{'}_m$


\item \todo{Kleene star}
\item Symbol count\\
The number of occurrences of $a$ in $w$, where $a \in \Sigma, w \in \Sigma^{*}$, noted as $|w|_{a}$.

\end{itemize}

\subsection{Grammars}
Linguists refer to grammar as a set of rules that specify how a natural language is formed. Due to the polysemous and vague nature of natural languages, it is not suited for the description of unambiguous systems. The inherit need to strictly describe languages introduced various language defining mechanisms. Many of these mechanisms are interchangeable, and may describe the same languages. How ever, not all mechanisms are able to describe languages formed by other mechanisms.

The concept of a grammar is a powerful tool for describing languages\cite[p. 52]{Linz2016Introduction}. A simple sentence in English consists of a single independent clause. A clause is typically formed by a subject and a predicate. This can be written as follows.
$$ \left< clause \right> \rightarrow \left< subject \right> \left< predicate \right> $$

We can further define the $\left< subject \right>$ and $\left< predicate \right>$. One of the possible subjects is a noun phrase, and the predicate may be a simple verb.
$$\left< subject \right>   \rightarrow   \left< determiner \right>   \left< premodifier \right>    \left< noun \right>    \left< postmodifier \right>$$
$$\left< predicate \right> \rightarrow \left< verb \right>$$

Associating the $\left< determiner \right>$ with the article "the", $\left< premodifier \right>$ with "fast" or "slow", $\left< noun \right>$ with "athlete", $\left< postmodifier \right>$ to "from England" or to an empty string and finally associating the $\left< verb \right>$ to "won" or "lost" allows us to write multiple sentences like "The fast athlete from England won" or "The slow athlete lost". These sentences are considered to be \emph{well formed}, as they resulted from following the grammatical rules.

The premise is consecutively replacing $\left< clause \right>$ until only irreducible blocks of the language remain. Generalizing this idea brings about the concept formal grammars.

\begin{definition}[Grammar]
\label{def:grammar}
\cite{Salomaa1987Formal}
Let an ordered quadruple $G$ define a grammar such that: $G=\left(N, \Sigma, P, S \right)$, where:
\begin{enumerate}
\item $N$ is a finite set of \emph{nonternminal} symbols
\item $\Sigma$ is an alphabet finite set of \emph{terminal} symbols, such that $N \cap \Sigma = \varnothing$
\item $P$ is a finite set of rewriting rules known as \emph{productions}, ordered pairs $\left( \alpha, \beta \right)$.
$P$ is a subset of the cartesian product of $\alpha = \left(N \cup \Sigma\right)^* N \left(N \cup \Sigma\right)^*$ and $\beta = \left(N \cup \Sigma\right)^*$


Productions are denoted as $\alpha \rightarrow \beta$.
If there are multiple productions with the same left hand side ($\alpha$), we can group their right hand sides ($\beta$).


$\alpha \rightarrow \beta_1, \alpha \rightarrow \beta_2$ may be written as $\alpha \rightarrow \beta_1 | \beta_2$

\item $S$ is the starting symbol of the grammar $G$, where $S \in N$
\end{enumerate}
\end{definition}

Productions $\alpha \rightarrow \beta$ symbolize, that given the words $V,W,x,y \in \left( N \cup \Sigma \right)^{*}$, the word $V$ can be rewritten as follows:
$V \Rightarrow W$ iff there are words, which satisfy the following condition, $V=x\alpha y \wedge W=x\beta y$ and $\alpha \rightarrow \beta \in P$.

\begin{definition}[Derivation]
\label{def:derivation}
$V \stackrel{*}{\Rightarrow}  W$ iff there is a finite set of words 
$$ v_0, v_1, v_2, \dots, v_z;\medskip z \in \mathbb{Z}$$
such that $v_0 = V$ and $v_z = W$ where each is rewritten from the previous word. Such a sequence of applications of productions is called a derivation.
The length of a derivation is given by $z$. 
\end{definition}

Grammars are often represented using formalisms, these formalisms often set restrictions on the left and right hand sides of productions. These restrictions further impose limits on the set of languages a grammar may produce, this is known as the \emph{expressive power} of a grammar. 

\subsection{Chomsky hierarchy}
When working with formal grammars, the need to compare their expressive power arose. Linguist Noam Chomsky introduced the now so called \emph{Chomsky hierarchy}\cite{chomsky1956three}. A set of four classes, each more expressive than the previous, see \cref{fig:chomsky-hierarchy}.



\begin{figure}[h]
  \label{fig:chomsky-hierarchy}
  \centering
  \includegraphics[width=0.75\textwidth]{figures/chomsky-hierarchy.pdf}
  \caption{Description of Chomsky hierarchy.}
\end{figure}

The intricaties of each class are not necessary in the context of this thesis, how ever relugar laguages will be used heavily throughout the rest of this thesis, so they will be explained more in depth.

\begin{table}[h]
\centering
\begin{tabular}{@{}ccl@{}}
\toprule
Grammar         & Language               & Restrictions \\ \midrule
$\mathcal{L}_3$ & Regular                & \todo{todo}             \\
$\mathcal{L}_2$ & Context Free           &              \\
$\mathcal{L}_1$ & Context Sensitive      &              \\
$\mathcal{L}_0$ & Recursively Enumerable &              \\ \bottomrule
\end{tabular}
\caption{An overview of the Chomsky hierarchy classes.}
\label{tab:chomsky-hierarchy}
\end{table}

\subsection{Regular Languages}
As shown above, regular languages are the inner most part of the Chomsky hierarchy, thus they are the most constricted. How ever, regular languages play a crutial role in lexical analysis, more precisely pattern matching. These constrictions lead to many useful closure propetries and decisability properties, mainly membership, which will be introduced shortly.

Regular languges may be defined in multiple equivalent manners such as through finite automata or regular expressions. 

\subsubsection{Finite Automata}

\begin{definition}[Deterministic Finite Automata]
\label{def:dfa}
DFAs are formally defined as 5-tuples
$$ M = (Q, \Sigma, \delta, q_0, F),$$
where
$Q$ is a finite set of states. $\Sigma$ is a finite set of symbols, also know as a input alphabet. $\delta$ is a transition function between states defined as $\delta : Q \times \Sigma \rightarrow Q$. $q_0$ is a initial state, $q_0 \in Q$. And $F$ is a set of final states, $F \subseteq Q$.

If starting in the state $q_0$ and moving left to right over the input string such that during each move a single symbol is consumed from the input string and a transition function for the corresponding state and symbol exists. If after all symbols from the input string have been read and the dfa stopped in a final state, the string is accepted. This is called a deterministic finite acceptor. \todo{rephrase nicer}
\end{definition}


\subsubsection{Regular Expressions} 

\begin{definition}[Regular Expressions]
  \label{def:regular_exp}
\end{definition}



\todo{show equivalence DFA = NFA = $\mathcal{L}_3$}



\begin{figure}[h]
  \label{fig:dfa}
  \centering
  \includegraphics[width=0.75\textwidth]{figures/dfa.pdf}
  \caption{Description of a compiler as a single unit.}
\end{figure}



  


\section{Compilers}


Today's world undeniably relies on software more than ever before. It is imperative for developers to be able to efficiently write software. Software today is written in programming languages, high-level human-readable notations for defining how a program should run. However, before a program can be run on a system, it has to be translated (or compiled) into low-level machine code, which computers can run. The computer program that facilitates this translation is called a \emph{compiler}. See \cref{fig:compiler}.

\begin{figure}[h]
  \label{fig:compiler}
  \centering
  \includegraphics[width=0.75\textwidth]{figures/compiler.pdf}
  \caption{Description of a compiler as a single unit.}
\end{figure}



Compilers are complex programs. It is helpful to break them down into parts, each handling different tasks, which are chained together to form a compiler. Modern compilers are composed of many phases, such as the Lexical Analyzer, Syntax Analyzer, Semantic Analyzer, Intermediate Code Generator, Machine-Independent Code Optimizer, Code Generator, Machine-Dependent Code Optimizer \cite[p. 5]{dragon}, however this chapter covers the components relevant to this thesis. See relevant stages in \cref{fig:compiler-stages}.


\begin{figure}[h]
 
  \label{fig:compiler-stages}
  \centering
  \includegraphics[width=0.75\textwidth]{figures/compiler-stages.pdf}
  \caption{Overview of relevant compiler stages.}
\end{figure}

\subsection*{Lexical Analysis}
The first stage of a compiler is lexical analysis, also known as a \emph{lexer} or \emph{tokenizer}. For the remainder of this thesis, \emph{lexer} will refer to the lexical analysis stage of a compiler. The lexer consumes a stream of characters, the \emph{source code}, and returns a stream of \emph{tokens}. A \emph{token} is a lexically indivisible unit, for example, the Python keyword \texttt{return}, you cannot divide it further, for example, into \texttt{re} \texttt{turn}. Each token is comprised of characters. The rule that defines which combination of characters constitutes a given token is called a \emph{pattern}. The sequence of characters matching a pattern is called a \emph{lexeme}, the \emph{lexeme} is stored along with the token as a value.

%Lexical analysis may be further divided into two distinct stages, however they are often implemented together. These stages are the \emph{scanner} and \emph{evaluator}.


\subsubsection*{Regular Expressions}
A compact way to represent the patterns accepting tokens are \emph{regular expressions}, which were introduced in \cref{def:regular_exp}. Regular expressions are an algebraic definition of patterns, they specify \emph{regular languages}, $\mathcal{L}_3$

\subsection*{Syntactic Analysis}

\subsubsection*{LL Parser}

\subsection*{Semantic Analysis}

\subsection*{Code Generation}


\chapter{Transpiler Implementation}
This chapter introduces the implementation of the source to source compiler designed to convert Lumerical's scripting language to Python combined with the Meep library. 
\section{Choice of Tools \todo{rename}}
When writing any program, it is important to choose the right tools for the job. That comes down not only to choosing a programming language but also what libraries and packages to utilize and which components to implement to better suite the project's constraints. The source language, Lumerical's scripting language allows you to automate tasks and analysis such as manipulating simulation objects, launching simulations, and analyzing results \cite{ansys_lsf}. This how ever is not suitable for implementing a transpiler. Looking at the target language, Python is a general-purpose high-level interpreted language. Writing the transpiler in Python will allow easier editing and debugging due to it's high-level nature. Following that, utilizing the fact that the implementation language is the same as the target language allows me to use built in modules for generating the target code. Python's ecosystem allow allows the use of tools such as Sphinx and Pytest for writing technical documentation and tests respectively.

\subsection{Sphinx}
Sphinx is a tool that automatically generates documentation by converting plain text source files into multiple output formats\cite{sphinx_quickstart}. Sphinx was chosen because it facilitates the extraction of docstring style comments from Python code with may be enriched by the addition of ReStructured Text \cite{docutils_rst}. Multiple output formats such as Portable Document Format (PDF), \LaTeX source code and Hypertext Markup Language (HTML) are supported. Sphinx also includes multiple extensions which allow the parsing of \LaTeX into Scalable Vector Graphics (SVG) which are easily rendered in the web.

This helps provide the user insight into the technical operation of the transpiler and allows him to search and view information in a concise manor.

\begin{figure}[h]

  \label{fig:compiler-stages}
  \centering
  \includegraphics[width=0.75\textwidth]{obrazky-figures/sphinx-lumex.png}
  \caption{Screen-capture of the resulting documentation.}
\end{figure}

\subsection{Pytest}
Pytest is a widely used testing framework for Python. 


\todo{mention python, sphinx, pytest, explain motivation for implementation of own transpiler}
\section{Grammar}
Before being able to translate between two languages, it is paramount to understand their grammar. Many languages provide definitions of grammars in a metasyntax format such as Backus-Naur form(BNF), extended Backus-Naus form(EBNF), Wirth syntax notation(WSN), augmented Backusâ€“Naur form (ABNF), parsing expression grammar (PEG) or Zephyr Abstract Syntax Definition Language (ASDL) \cite{asdl}. Python grammar is defined by a mixture of EBNF and PEG as used in the CPython parser \cite{python3grammar}. Additionally Python abstract symbol tree (AST) module also defines an abstract grammar in ASDL\cite{python_ast}.

Lumerical, as of writing this thesis, does not publicly provide any such specification for Lumerical's scripting language. Thus it is necessary to analyze the language and derive such a grammar at least for the scope of this thesis.

\todo{provide steps to reverse-engineer grammar}
\todo{add grammar listing here or in appendix?}



\section{Lexical Analysis}
After defining a grammar, the first stage in creating a transpiler is the lexer. There are multiple methods in which a lexer may be modeled, the most straight forward is with the use of a DFA formalism and a loop over all characters. Always checking if there is a valid production from the current character to the look-ahead character and greedily continuing and backtracking to the last successful match. 


\begin{algorithm}[H]

current\_state $\gets$ \texttt{None}\\
last\_accepting\_state $\gets$ \texttt{None}

    \While{lookahead\_character != EOF}{
        \If{has\_transition(current\_state, lookahead)}{
            current\_state $\gets$ \text{next\_state}(current\_state, lookahead)\\
            get\_next\_character()
            
            \If{\text{is\_accepting\_state}(current\_state)}{
                last\_accepting\_state $\gets$ current\_state
            }
        }
        \Else{
            \If{no\_accepting\_state\_visited}{

				\Return{\texttt{None}} \tcp*{Failure} 
            }
            \Else{
				revert\_to\_previous\_accepting\_state\_and\_revert\_input\_characters
				\Return{current\_state} \tcp*{Success} 
            }
        }
    }
    


\caption{State Machine Based Lexer}
\label{alg:lexer}
\end{algorithm}

This implementation is fine, how ever with an ever growing number of states, which have to be hard-coded, the solution may become rather complex. How ever there is a cleaner method. Since DFA may be transformed into regular expressions, it is sufficient to try to match all regular expressions against the input and return the longest match while removing it from the input.


\begin{algorithm}[H]

longest\_match $\gets$ \texttt{None}\\


    \For{all\_regular\_expressions}{
        \If{regular\_expression\_matches\_input}{
            \If{regular\_expression\_is\_longer\_than\_longest\_match}{
                longest\_match $\gets$ current\_regular\_expression
            }
        }
    }
    \If{longest\_match != \texttt{None}}{
		\Return{matching\token}    
		remove\_matched\_lexeme\_from\_input
    }
    \Else{
    		\Return{\texttt{None}} \tcp*{Failure} 
    }
    


\caption{Regular Expression Based Lexer}
\label{alg:lexer}
\end{algorithm}


\todo{mention lexer regex implementation}
\section{LL Parser}
\todo{mention LL table calculation according to theory}
\section{Code Generation}
\todo{}
\todo{handling context specific actions such as the selection -> introduction of runtime class}
\todo{write about transformation of ast}



\chapter{Evaluation of Results}
\todo{talk about methodology}
\todo{show comparison of leumerical code, generated python and handwritten -> conclusion probably not best method}
\todo{test examples analytic vs FDTD}
\chapter{Conclusion}
\todo{compare lumerical and meep}
\todo{test result TBD}




%=========================================================================

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}
